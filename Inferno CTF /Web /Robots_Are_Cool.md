# Challenge "Robots Are Cool"

<img width="496" height="567" alt="image" src="https://github.com/user-attachments/assets/bbb7da6f-3c15-4325-91e2-45b7a72bddc1" />

____________________________________________________________
- Host locally with Docker:

<img width="1900" height="236" alt="image" src="https://github.com/user-attachments/assets/d1ba9fec-d1ec-4c3a-9ea6-e8151d0df7e6" />


- Access the website locally at `localhost:8080` :

<img width="688" height="154" alt="image" src="https://github.com/user-attachments/assets/ee4925a3-03ab-4789-99d1-21ebacc665e8" />

- Lets check `localhost:8080/robots.txt` :

Think of the internet as a giant city, and websites are the buildings. Search engines like Google have robots (also called crawlers) that go around visiting every building to see what’s inside so they can show it in search results.

`/robots.txt` is just a file at the front of a website that tells these robots:

- Where they can go (like “this door is open” 🚪)
 
- Where they shouldn’t go (like “please don’t go in this door” 🚫)
______________________________________________

- On our Case let's access `localhost:8080/robots.txt` :

<img width="300" height="54" alt="image" src="https://github.com/user-attachments/assets/895d4e7a-e2f7-48d5-9a30-8f8d97bcb0cd" />

Flag : `flag{r0b0t5_4r3_c00000l_r1ght}`

- Let's access now as we saw in the picture that hidden Directory at http://localhost:8080/supersecretpage.html :

<img width="424" height="143" alt="image" src="https://github.com/user-attachments/assets/33f1ee0b-4304-4386-b59b-e2fe2afab798" />

